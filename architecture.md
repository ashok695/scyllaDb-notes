# ðŸš€ ScyllaDB Concepts - A Fun Theory Class!

> **Gonna be a theory class!!!** Don't be bored, or make your face sleepy ðŸ˜´ -> *joke*

I'm gonna try to make this interesting! ðŸ˜†

---

## ðŸ›  General Concepts:

### ðŸ” Replication:
Making multiple copies of data spread across nodes to **prevent data loss**.

### ðŸ”€ Sharding:
Dividing data into smaller parts (**shards**) and storing across various nodes.

ðŸ’¡ *These two are database techniques used for improving scalability, availability, and fault tolerance.*

### ðŸ“¦ Node:
A **database instance** used to store the data.

### ðŸ¢ Cluster:
A **group of nodes** forming a cluster to store and replicate data.

---

## ðŸ—ï¸ ScyllaDB Architecture:

![Architecture Diagram](/images/architecture.png)

### 1ï¸âƒ£ Shared Nothing Architecture
ScyllaDB is a **shared nothing architecture**, meaning there is **no single point of failure**.

### ðŸ”¥ What Does Shared Nothing Mean?
Users can create either a **single node** or **multiple nodes**.

#### Single Node:
- Entire data is stored **on that node**.
- Using **sharding**, data is spread across the **available CPU cores**.
- Since this is a single node, **no replication happens**.

#### Multiple Nodes (Example: 3 Nodes):
- Data is **partitioned and stored across nodes**.
- Each node **manages its own data**.
- **Sharding still happens within a node.**
- When replication is enabled, copies of data (replicas) are stored across nodes for **higher availability and performance**.
- **No shared memory, CPU, or disk space. Each node is independent.**

---

## ðŸ§© Sharding:

ðŸ’¡ *We already know that sharding splits data into smaller chunks.*

### How Does Sharding Work?
Imagine we have **10 rows** to store in a **single node**.

âŒ Instead of storing all **data in a single CPU core**,
âœ… We **split** data into **10 chunks** and store them across **different cores**.

ðŸ’¡ *Why is this helpful?*
If a user asks for **one row**, instead of scanning the entire table, we can just fetch the row **directly from the specific core**. ðŸ”¥

### Example:
Imagine we have **10 tasks**. We know we need to **shard** the data. But **how?** ðŸ¤”

1ï¸âƒ£ Identify the **Primary Key** â€“ This is a **unique identifier** for each row. In our case, `_id` is unique.
2ï¸âƒ£ Convert the **Primary Key** into a **Token** (using hashing techniques).
3ï¸âƒ£ ScyllaDB decides **which core stores which data** based on **token range** and **available cores**.
4ï¸âƒ£ **Sharding is fully handled by ScyllaDB!** ðŸŽ‰

### Visual Representation of sharding:
![Architecture Diagram](/images/sharding.png)

---

## ðŸ” Replication:
Replication ensures **higher availability** and **fault tolerance** by storing **multiple copies** of data across nodes.

### Key Components of Replication:

#### 1ï¸âƒ£ **Replication Factor**
The number of copies stored across different nodes.
If one node **fails**, other nodes serve the data.

#### 2ï¸âƒ£ **Replication Strategies**
- **Simple Strategy** ðŸ¢
  - Contains **only one data center**.
  - Data is replicated across **all nodes** in that cluster.

- **Network Topology Strategy** ðŸŒ
  - **Multiple data centers**, each with its own cluster.
  - Each cluster contains **multiple nodes**.
  - We can specify **different replication factors** for each data center.

ðŸ“Œ *Example:* Imagine we have a **data center in the USA** and one in **India**.
- **USA DC** â†’ **4 replicas**
- **India DC** â†’ **3 replicas**

Now, data is **replicated separately** in each location based on the required **availability & redundancy**.

---

## ðŸ”„ Consistency Levels:

We have made copies of data and spread them across nodes, right? Now, we tell ScyllaDB how many copies it refers to during **reads/writes**.

### Types of Consistency Levels:

- **ONE** â†’ Fastest, but accuracy is low because it refers to only one copy.
- **QUORUM** â†’ A balance between speed and accuracy.
- **ALL** â†’ Slowest, but ensures data correctness by referring to all copies.

---

 **TOO MUCH THEORY, RIGHT?!**
Let's jump into **practical examples**. ðŸš€ðŸ”¥
next -> CQL.md file